# -*- coding: utf-8 -*-
"""Nested Classifier Prototyping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QWn3Av1x1QXF8psuXp8tgLlcS4ruF9O1
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.classify import NaiveBayesClassifier
import pandas as pd
from google.colab import files

nltk.download('punkt')
nltk.download('stopwords')

def extract_features(text):
    tokens = word_tokenize(text.lower())  # Tokenize and lowercase
    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation
    stop_words = set(stopwords.words('english'))  # Define stop words
    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words
    return {word: True for word in tokens}  # Create feature set

labeled_texts = [
    ("this is a medical document about heart disease", "medical"),
    ("this is a general news article about technology", "non-medical"),
    ("" ,"")]

med_df = pd.read_csv("med_texts.csv")
nonmed_df = pd.read_csv("nonmed_texts.csv")

nonmed_df = nonmed_df.iloc[:len(med_df)]

combined_df = pd.concat([med_df, nonmed_df], ignore_index=True)

med_vs_nonmed_df = combined_df.sample(frac=1).reset_index(drop=True)

med_vs_nonmed_df

med_vs_nonmed_df = med_vs_nonmed_df.dropna(subset=['text'])

print(med_vs_nonmed_df['text'].apply(type).value_counts())

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.classify import NaiveBayesClassifier
from nltk.classify.util import accuracy

nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text.lower())
    # Remove stopwords
    tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]
    return tokens

med_vs_nonmed_df['text'] = med_vs_nonmed_df['text'].apply(preprocess_text)

def extract_features(tokens):
    return {word: True for word in tokens}

feature_sets = [(extract_features(row['text']), row['label']) for index, row in med_vs_nonmed_df.iterrows()]

train_size = int(len(feature_sets) * 0.8)  # Use 80% for training
train_set = feature_sets[:train_size]
test_set = feature_sets[train_size:]

classifier = NaiveBayesClassifier.train(train_set)

import pickle

with open('naive_bayes_classifier.pkl', 'wb') as f:
    pickle.dump(classifier, f)

print("Accuracy:", accuracy(classifier, test_set))

new_text = """Volkswagen, Germany’s biggest employer, is thinking about cutting many jobs. This is because car sales in Germany, including those from Mercedes-Benz and BMW, are falling.

Although Volkswagen denied reports of laying off 30,000 workers in Germany, it still says it needs to reduce its costs. Germany’s Economy Minister, Robert Habeck, offered help, but Volkswagen needs to decide how to solve its problems.

The main reason for this situation is a drop in sales in China, which used to be a very big market for German car companies. Now, China’s car market is not as strong, and there are more Chinese car makers. The German government may help, but Volkswagen will still need to make some difficult decisions. protected places. It helps nature, tourism, and clean air and water."""

processed_new_text = preprocess_text(new_text)
features_new_text = extract_features(processed_new_text)

classification = classifier.classify(features_new_text)

classification

